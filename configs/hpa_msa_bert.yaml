# configs/hpa_msa_bert.yaml

# =================================================================
# Configuration for HPA-MSA with End-to-End BERT Fine-tuning
# =================================================================

# --- I. General Settings ---
project_name: 'HPA_MSA_BERT_CMU-MOSI'
dataset_name: 'CMU-MOSI'
seed: 42
use_gpu: true

# --- II. Path and Data Settings ---
data_path: 'data/CMU-MOSI/processed/aligned_50.pkl' # This file must contain 'raw_text'
checkpoints_path: 'checkpoints'
num_workers: 4

# --- III. Model Architecture Hyperparameters ---

# A. BERT Specific Settings
bert_model_name: 'bert-base-uncased'
max_text_len: 52 # Max sequence length for tokenizer, including [CLS] and [SEP]

# B. Non-Text Feature Dimensions
audio_dim: 5
vision_dim: 20

# C. HPA-MSA Core Dimensions
d_model: 128
num_classes: 2 # For binary classification

# D. UEICD Module Settings
ueicd_nhead: 8
ueicd_nlayers: 2
ueicd_dropout: 0.1

# E. CACE Module Settings
cace_nhead: 8
cace_dropout: 0.1

# --- IV. Training Hyperparameters ---
batch_size: 16 # Fine-tuning BERT often requires a smaller batch size
num_epochs: 10
learning_rate: 0.00002 # A smaller learning rate is crucial for fine-tuning
weight_decay: 0.01
grad_clip: 1.0

# V. Loss Function Weights
lambda_ortho: 0.5
lambda_mi: 0.2

# --- VI. Training Mode Control ---
train_with_regression_only: true
label_smoothing: 0.0 # Typically not used when fine-tuning with small LR